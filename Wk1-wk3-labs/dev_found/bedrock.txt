import boto3
import json

# Step 1: Connect to Bedrock
brt = boto3.client("bedrock-runtime")

# Step 2: Ask the user for inputs
ingredients = input("Enter the ingredients you have at home (comma-separated): ")
pet_type = input("Enter the type of pet (e.g., 'dog', 'cat'): ")
allergies = input("Enter any allergies your pet has (or type 'none'): ")

# Step 3: Dynamically create the prompt using f-strings
prompt = f"Create a recipe suggestion for a {pet_type} using {ingredients}. Avoid any ingredients they are allergic to: {allergies}."

# Step 4: Send the prompt to Bedrock and generate the recipe
request_payload = json.dumps({"inputText": prompt})
response = brt.invoke_model(modelId="amazon.titan-text-express-v1", body=request_payload)

# Step 5: Process and display the recipe
model_response = json.loads(response["body"].read())
recipe = model_response["results"][0]["outputText"]
print(f"\nGenerated Recipe for {pet_type}: {recipe}")


#SPECIFYING INFERENCE TEMPERATURE
# Import necessary libraries.
import boto3
import json

# Create a Bedrock runtime client.
bedrock_runtime = boto3.client("bedrock-runtime")

# Define the model ID and input prompt.
model_id = "amazon.titan-text-express-v1"
prompt = "Generate a catchy slogan for a pet store specializing in healthy pet food."

# Specify inference parameters, including temperature for creativity control.
parameters = {
    "temperature": 0.7  # A mid-level value for balanced creativity
}

# Format the request payload using the model's native structure.
native_request = {
    "inputText": prompt,
    "textGenerationConfig": parameters
}

# Send the request to the Bedrock model.
response = bedrock_runtime.invoke_model(
    modelId=model_id,
    body=json.dumps(native_request)
)

# Parse and display the response.
response_body = json.loads(response["body"].read())
print("Generated Slogan:", response_body["results"][0]["outputText"])